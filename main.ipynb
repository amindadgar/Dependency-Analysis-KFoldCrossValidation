{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c1ac15cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79c7473",
   "metadata": {},
   "source": [
    "# Machine Learning Final Project\n",
    "**Student name:** Mohammad Amin Dadgar\n",
    "\n",
    "**Student Id:** 4003624016\n",
    "\n",
    "**Instructor:** Dr. Peyman Adibi\n",
    "\n",
    "**Date:** Tir 1401 | June, July 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d40454",
   "metadata": {},
   "source": [
    "The Final Project is the implementation of [dependency analysis of accuracy estimates in k-fold cross validation](https://ieeexplore.ieee.org/document/8012491) article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8167f86",
   "metadata": {},
   "source": [
    "**Abstract:** K-fold cross-validation is a method to evaluate the performance of classification algorithms. In this report, we are going to show the appropriateness of K-fold cross-validation for the dependence of fold accuracies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9462ab23",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "Several studies have shown that in k-fold cross-validation, fold accuracies are dependent, but there is no formal definition for this fact. This report is the reproduction results of referenced article that in section 2, we introduced the statistical methods needed for this experiment. In section 3 sampling the distribution from fold accuracies is introduced and using this in section 4, a statistical method is shown to evaluate the fold accuracies independence. In section 5, the method introduced in section 4 is tested on 20 UCI datasets, and in section 6 weâ€™ve concluded the work done before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a842e43",
   "metadata": {},
   "source": [
    "To inference the idea behind this article, in section 4 the variances are given as below\n",
    "\n",
    "sample variance:\n",
    "\\begin{equation}\n",
    "s^2 = \\frac{\\sum_{i=0}^{k} (\\bar p_i - \\bar{\\bar{x}})^2}{k-1}\n",
    "\\end{equation}\n",
    "And the variance for leave-one-out cross validation\n",
    "\\begin{equation}\n",
    "\\sigma_I^2 = \\frac{p(1-p)}{n}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ea8c9",
   "metadata": {},
   "source": [
    "to explain the equations above we can say that $p$ is the accuracy of the classifier and to find it, we can devide the correct classifications by the sample size.\n",
    "\n",
    "$\\bar p_i$ refers to the accuracy estimate in fold i, and can be written as below\n",
    "\\begin{equation}\n",
    "\\bar{p_i} = \\frac{\\sum_{j=1}^{m} x_{ij}}{m}\n",
    "\\end{equation}\n",
    "where $m$ is the count of each fold sample. For example if we had $200$ sample in our dataset and using $5$ folds, then we would have $m=\\frac{200}{5}$.\n",
    "\n",
    "where $x_{ij}$ is a function that outputs $1$ where the classification is correct and outputs $0$ when the classification is wrong.\n",
    "\n",
    "The $\\bar{\\bar{x}}$ represents the sample mean and it's possible to find it using the equation below\n",
    "\\begin{equation}\n",
    "\\bar{\\bar{x}} = \\frac{\\sum_{i=1}^{k} \\bar{p_i}}{k}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf6016a",
   "metadata": {},
   "source": [
    "We will start the dependency test of folds with a hypothesis $H_0: s^2/k=\\sigma_l^2$. the test statistics can be found as $\\chi^2 = \\frac{(k-1) s^2}{k \\sigma_l^2}$ when $\\chi$ has $k-1$ degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "953170ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8\n",
      "acc: 0.75\n",
      "acc: 0.675\n",
      "acc: 0.75\n",
      "acc: 0.625\n",
      "sample mean is 0.72\n"
     ]
    }
   ],
   "source": [
    "######### Let's re-calculate the example 2 and 3 in the article\n",
    "total = 200\n",
    "classification_res = np.array([32, 30, 27, 30, 25])\n",
    "sample_mean = np.sum(classification_res / 200) \n",
    "\n",
    "for true_classified in classification_res:\n",
    "    print(f\"acc: {true_classified / 40}\")\n",
    "\n",
    "print(f\"sample mean is {sample_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c4ddeb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### implementing equation 1\n",
    "def find_sample_variance(true_classification, fold_samples_count):\n",
    "    \"\"\"\n",
    "    Find sample variance of k-fold cross validation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    true_classification : array of floats\n",
    "        the portion of true classified samples in each fold\n",
    "        note that the folds count is the length of this array\n",
    "    fold_samples_count : positive integer\n",
    "        the count of folds data samples\n",
    "        \n",
    "    Returns:\n",
    "    ---------\n",
    "    sample_variance : float\n",
    "        the sample variance of all the folds\n",
    "    \"\"\"\n",
    "    ## find the total sample count\n",
    "    total_sample_count = fold_samples_count * len(true_classification)\n",
    "    \n",
    "    sample_mean = find_sample_mean(true_classification, total_sample_count)\n",
    "    \n",
    "    ## the subtraction of true classified portion from sample mean\n",
    "    subtraction_arr = np.subtract(true_classification / fold_samples_count, sample_mean)\n",
    "\n",
    "    ## to divide the found value by `k-1`\n",
    "    sample_variance = np.sum(np.power(subtraction_arr, 2)) / (len(true_classification) - 1)\n",
    "    \n",
    "    return sample_variance\n",
    "    \n",
    "def find_sample_mean(true_classification, total_samples_count):\n",
    "    \"\"\"\n",
    "    Find the sample mean of k-fold cross validation using the true classification results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    true_classification : array of floats\n",
    "        the portion of true classified samples in each fold\n",
    "        note that the folds count is the length of this array\n",
    "    total_samples_count : positive integer\n",
    "        the count of total sample size (dataset length maybe)\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    sample_mean : float\n",
    "        the sample mean calculated for all the dataset\n",
    "    \"\"\"\n",
    "    sample_mean = np.sum(true_classification / total_samples_count)\n",
    "    \n",
    "    return sample_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5b6f8417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004812500000000001"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_sample_variance = find_sample_variance(classification_res, 40)\n",
    "m_sample_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "521449ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_total_variance(accuracy, total_sample_count):\n",
    "    \"\"\"\n",
    "    Find the variance of a set using its accuracy and samples count\n",
    "    the equation is `accuracy(1-accuracy)/total_sample_count`\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    accuracy : float between 0 and 1\n",
    "        the floating value that rerpresent the portion of true classified samples over all samples\n",
    "    total_sample_count : integer\n",
    "        the total samples in a dataset\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    variance : float\n",
    "        the calculated variance for the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    variance = (accuracy * (1 - accuracy)) / total_sample_count\n",
    "    \n",
    "    return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "471162f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010500000000000002"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_variance = find_total_variance(0.7, 200)\n",
    "m_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "48f83552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_independence_test(k_folds, total_variance, sample_variance):\n",
    "    \"\"\"\n",
    "    the chi square independence test introduced in the article\n",
    "    equation is `(`k-1` folds * sample_varience) / `k` folds  * total_variance`\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    k_folds : positive integer\n",
    "        the count of folds applied for a model\n",
    "    total_variance : float\n",
    "        the variance represented by leave-one-out cross validation\n",
    "    sample_variance : float\n",
    "        the variance that is found by aggregation of fold accuracies\n",
    "        \n",
    "    Returns:\n",
    "    ---------\n",
    "    chi_square : float\n",
    "        the chi square value \n",
    "    \"\"\"\n",
    "    \n",
    "    chi_square = ((k_folds - 1) * sample_variance) / (k_folds * total_variance)\n",
    "    \n",
    "    return chi_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a595407a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6666666666666665"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chisquare_value = chi_independence_test(5, m_variance, m_sample_variance)\n",
    "chisquare_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954fa7ba",
   "metadata": {},
   "source": [
    "And finding the exact values for `Example 3.`, we can now go on to real tests for real datasets, but Before going to experiments in real datasets another thing is to find out the p-value for our test.\n",
    "\n",
    "So we are going to find the p-value first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c48f77b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5470073861075335"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.chi2.cdf(chisquare_value, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a000f66",
   "metadata": {},
   "source": [
    "## 1-NN\n",
    "Let's try KNN method with K=1 with different datasets. minkowski distance with $p=2$ is the euclidean distance and we are using euclidean distance as our nearest neighbour metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8bcc9670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, LeaveOneOut, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab72fee9",
   "metadata": {},
   "source": [
    "### liver-disorders dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bad73a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mcv</th>\n",
       "      <th>alkphos</th>\n",
       "      <th>sgpt</th>\n",
       "      <th>sgot</th>\n",
       "      <th>gammagt</th>\n",
       "      <th>drinks</th>\n",
       "      <th>selector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85</td>\n",
       "      <td>92</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>64</td>\n",
       "      <td>59</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86</td>\n",
       "      <td>54</td>\n",
       "      <td>33</td>\n",
       "      <td>16</td>\n",
       "      <td>54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91</td>\n",
       "      <td>78</td>\n",
       "      <td>34</td>\n",
       "      <td>24</td>\n",
       "      <td>36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87</td>\n",
       "      <td>70</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mcv  alkphos  sgpt  sgot  gammagt  drinks  selector\n",
       "0   85       92    45    27       31     0.0         1\n",
       "1   85       64    59    32       23     0.0         2\n",
       "2   86       54    33    16       54     0.0         2\n",
       "3   91       78    34    24       36     0.0         2\n",
       "4   87       70    12    28       10     0.0         2"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_liver = pd.read_csv('Datasets/liver-disorders/bupa.data', names=['mcv', 'alkphos', 'sgpt', 'sgot', 'gammagt', 'drinks', 'selector'])\n",
    "ds_liver.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "440b013b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2], dtype=int64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_liver.selector.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a1f88f",
   "metadata": {},
   "source": [
    "we have two classess for each data, selector is the label in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e339c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_1_liver = KNeighborsClassifier(n_neighbors=1, p=2)\n",
    "# KNN_1_liver.fit(ds_liver[ds_liver.columns[:-1]], ds_liver.selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f6a4cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_liver_X = ds_liver[ds_liver.columns[:-1]]\n",
    "ds_liver_Y = ds_liver.selector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d3f9d1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.0026772 , 0.00232077, 0.00202894, 0.00267959, 0.00164056]),\n",
       " 'score_time': array([0.00377774, 0.00390458, 0.00401592, 0.00382686, 0.00284386]),\n",
       " 'estimator': [KNeighborsClassifier(n_neighbors=1),\n",
       "  KNeighborsClassifier(n_neighbors=1),\n",
       "  KNeighborsClassifier(n_neighbors=1),\n",
       "  KNeighborsClassifier(n_neighbors=1),\n",
       "  KNeighborsClassifier(n_neighbors=1)],\n",
       " 'test_score': array([0.65217391, 0.76811594, 0.62318841, 0.66666667, 0.53623188]),\n",
       " 'train_score': array([1., 1., 1., 1., 1.])}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN_1_5cv_scores = cross_validate(KNN_1_liver, \n",
    "                                  ds_liver_X,\n",
    "                                  ds_liver_Y,\n",
    "                                  cv=5,\n",
    "                                  return_train_score=True,\n",
    "                                 return_estimator=True)\n",
    "KNN_1_5cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3c7fd1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6492753623188406"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_sample_mean = np.mean(KNN_1_5cv_scores['test_score'])\n",
    "m_sample_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "05aa0982",
   "metadata": {},
   "outputs": [],
   "source": [
    "## find sample variance\n",
    "def find_sample_variance_using_accuracies(fold_accuracies):\n",
    "    \"\"\"\n",
    "    Find variances using the accuracies got from k-fold cross validation\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    fold_accuracies : float between 0 and 1\n",
    "        the accuracies in each fold of k-fold cross validation\n",
    "        note that the k folds count will be computed using the length of this parameter  \n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    variance : float\n",
    "        the variance of accuracies\n",
    "    \"\"\"\n",
    "    sample_mean = np.mean(fold_accuracies)\n",
    "    \n",
    "    ## the subtraction of true classified portion from sample mean\n",
    "    subtraction = np.subtract(fold_accuracies, sample_mean)\n",
    "    \n",
    "    variance = np.sum(np.power(subtraction, 2)) / (len(fold_accuracies) - 1)\n",
    "    \n",
    "    return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "44df1246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006973324931737026"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_variance = find_sample_variance_using_accuracies(KNN_1_5cv_scores['test_score'])\n",
    "sample_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b57b68",
   "metadata": {},
   "source": [
    "#### Leave one out method\n",
    "In this method the folds count is eqaul to the data size and test size in each fold is 1.https://www.statology.org/leave-one-out-cross-validation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "93924ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_leaveOneOut = KNeighborsClassifier(n_neighbors=1, p=2)\n",
    "KNN_leaveOneOut_result = cross_validate(KNN_leaveOneOut, \n",
    "                                  ds_liver_X,\n",
    "                                  ds_liver_Y,\n",
    "                                  cv=LeaveOneOut(),\n",
    "                                  return_train_score=True,\n",
    "                                 return_estimator=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ea8a77e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
       "       1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 1., 1., 0.])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN_leaveOneOut_result['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ca3d2b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6202898550724638"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaveOneOut_acc = sum(KNN_leaveOneOut_result['test_score']) / len(ds_liver)\n",
    "leaveOneOut_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "51f60c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implementing leave-one-out method by hand\n",
    "kf = KFold(n_splits=len(ds_liver))\n",
    "\n",
    "KNN_leaveOneOut = KNeighborsClassifier(n_neighbors=1, p=2)\n",
    "\n",
    "\n",
    "test_result = []\n",
    "for train, test in kf.split(ds_liver):\n",
    "    ## predict using the classifier\n",
    "    KNN_leaveOneOut.fit(ds_liver_X.loc[train], ds_liver_Y.loc[train])\n",
    "    ## predict the test data\n",
    "    y_pred = KNN_leaveOneOut.predict(ds_liver_X.loc[test])\n",
    "    \n",
    "    test_result.append(y_pred == ds_liver_Y.loc[test].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "717fb117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62028986])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_result) / len(ds_liver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c7acf",
   "metadata": {},
   "source": [
    "We can see that the accuracy of leave-one-out compited by hand is the same as the `LeaveOneOut()` method in sklearn. So we will continue with sklearn method because of its simplicity. (We just implement it by hand to see we are going in the right direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6ccbe4",
   "metadata": {},
   "source": [
    "Now as Example 3, we can compute the chi value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7fc62144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000682696668888828"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma = (leaveOneOut_acc * (1 - leaveOneOut_acc)) / len(ds_liver)\n",
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "51138296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.171506028394088"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the folds was chosen 5 above\n",
    "chi_square = ((5-1) * sample_variance) / (5* sigma)\n",
    "chi_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2dcf69d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9145060709028664"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## number of freedom is one less than 5\n",
    "stats.chi2.cdf(chi_square, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cea600",
   "metadata": {},
   "source": [
    "With significance level $\\alpha = 0.05$, our null hypothesis cannot be reject because the p-value is $0.91$. And we can say that here 5 fold accuracies are independent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f7b17f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
